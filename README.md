# Laboratorio 1: Multi-Armed Bandits con Algoritmo Epsilon-Greedy

Este laboratorio implementa el algoritmo epsilon-greedy para resolver el problema de multi-armed bandits, con el objetivo de maximizar la recompensa acumulativa a lo largo de m√∫ltiples iteraciones.

## üìã Descripci√≥n del Problema

El problema de multi-armed bandits es un paradigma cl√°sico en el aprendizaje por refuerzo donde un agente debe decidir entre m√∫ltiples acciones (brazos) sin conocer inicialmente las probabilidades de recompensa de cada uno. El desaf√≠o consiste en equilibrar la **exploraci√≥n** (probar brazos desconocidos) y la **explotaci√≥n** (usar el conocimiento actual para maximizar recompensas).


## üöÄ Implementaci√≥n

### Clases Principales

#### 1. Clase `Bandit`
Representa el entorno de 10 brazos:
- **Inicializaci√≥n**: Genera probabilidades aleatorias para cada brazo (0-1)
- **M√©todo `pull(arm)`**: Devuelve recompensa de 1 con probabilidad espec√≠fica del brazo, 0 en caso contrario

#### 2. Clase `Agent`
Implementa la estrategia epsilon-greedy:
- **Par√°metros**:
  - `k`: N√∫mero de brazos (10)
  - `epsilon`: Probabilidad de exploraci√≥n
  - `counts`: Contador de extracciones por brazo
  - `values`: Valores estimados (Q) para cada brazo

- **M√©todos**:
  - `select_arm()`: Selecci√≥n de brazo con estrategia Œµ-greedy
  - `update_estimates()`: Actualizaci√≥n de estimaciones usando media incremental

### Algoritmo Epsilon-Greedy

```
Con probabilidad Œµ: EXPLORAR (brazo aleatorio)
Con probabilidad 1-Œµ: EXPLOTAR (brazo con mayor Q estimado)
```

### Actualizaci√≥n de Estimaciones

```
Q ‚Üê Q + (1/n) √ó (reward - Q)
```

Donde:
- `Q`: Valor estimado actual
- `n`: N√∫mero de veces que se ha seleccionado el brazo
- `reward`: Recompensa obtenida

## üìä Experimentos y Resultados

### Configuraci√≥n Base
- **N√∫mero de brazos**: 10
- **Iteraciones**: 1,000
- **Epsilon inicial**: 0.1
- **Semilla**: 42 (para reproducibilidad)

### Experimentos con Diferentes Valores de Epsilon

| Epsilon | Comportamiento | Caracter√≠sticas |
|---------|----------------|-----------------|
| 0.01    | Mayor explotaci√≥n | Convergencia r√°pida, menor exploraci√≥n |
| 0.1     | Balance equilibrado | Buen compromiso exploraci√≥n-explotaci√≥n |
| 0.5     | Mayor exploraci√≥n | M√°s exploraci√≥n, convergencia m√°s lenta |

## üìà Visualizaciones

El notebook incluye las siguientes gr√°ficas:

1. **Recompensa Acumulada vs Iteraciones**: Muestra el progreso del aprendizaje
2. **Valores Estimados vs Probabilidades Reales**: Eval√∫a la precisi√≥n de las estimaciones
3. **Comparaci√≥n entre diferentes valores de epsilon**: An√°lisis del impacto del par√°metro de exploraci√≥n

